%!TEX root = main.tex
\section{Introduction}
\label{sec:intro}

Data storage is no longer about writing data to a single
disk with a single point of access. Modern applications require not just data
reliability, but also high-throughput concurrent accesses. 
Applications concerning supply chains, banking, etc. use traditional relational databases
for storing and processing data, whereas applications such as social networking
software and e-commerce platforms 
use cloud-based storage systems (such as Azure Cosmos DB \cite{cosmosdb}, Amazon DynamoDB
\cite{decandia2007dynamo}, Facebook TAO \cite{facebook-tao}, etc.). 
%We use the term \textit{storage system} in this paper to refer to any such database system or service.


%As applications have moved from a single-box environment to the cloud, the notion of
%data persistence has also changed. It is no longer about storing data on a
%single disk with a single point of access. Rather, modern applications such as
%social networking software, e-commerce platforms, cloud micro-services, etc. are built using 
%high-scale storage systems, such as Azure CosmosDb \cite{cosmosdb}, Amazon DynamoDb \cite{amazon-dynamodb}, 
%Facebook TAO \cite{facebook-tao}. Applications such as 
 
%These storage systems, commonly offered by most major cloud providers (such as
%Azure CosmosDb \cite{cosmosdb}, Amazon DynamoDb \cite{amazon-dynamodb}, 
%Facebook TAO \cite{facebook-tao}, etc.)
%create and manage multiple replicas of data. Having multiple replicas offers reliability and prevents
%data loss, but it also offers availability and low-latency accesses by allowing
%different clients to connect with different replicas. 

Providing high-throughput processing, unfortunately, comes at an unavoidable cost of weakening 
the guarantees offered to users.
Concurrently-connected clients may end up observing different views of the same data. 
These ``anomalies'' can be prevented by using a strong \textit{isolation level} 
such as \textit{Serializability} \cite{DBLP:journals/jacm/Papadimitriou79b}, which essentially offers a single view of the
data. However, serializability requires expensive synchronization and incurs a high performance cost. As a
consequence, most storage systems use weaker isolation levels, such as 
{\it Causal Consistency}~\cite{DBLP:journals/cacm/Lamport78,DBLP:conf/sosp/LloydFKA11,antidote},
{\it Snapshot Isolation}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, {\it Read
Committed}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, etc. for better performance.
In a recent survey of
database administrators \cite{survey}, 86\% of the participants responded that
most or all of the transactions in their databases execute at Read Committed isolation level.

A weaker isolation level allows for more possible behaviors than stronger
isolation levels. It is up to the developers then to ensure that their
application can tolerate this larger set of behaviors. Unfortunately, weak
isolation levels are hard to understand or reason about
\cite{DBLP:conf/popl/BrutschyD0V17,adya-thesis} and resulting application bugs
can cause loss of business \cite{acidrain}.

\paragraph{Model Checking Database-Backed Applications}
This paper addresses the problem of \textit{model checking} code for correctness against a given isolation level. 
%against weak behaviors: a developer should be able to write a test that runs
%their application and then asserts for correct behavior. 
%The main difficulty with testing today is getting coverage of weak behaviors.
\emph{Model checking}~\cite{DBLP:conf/popl/ClarkeES83,DBLP:conf/programm/QueilleS82} explores the state space of a given program in a systematic manner and verifies that each reachable state satisfies a given property. It provides high coverage of program behavior, but it faces the infamous state explosion problem, i.e., the number of possible executions grows exponentially in the size of the source code. 
%In this paper, we consider shared-memory programs running on a sequentially consistent memory model, for which interleavings of atomic steps in different threads are a precise model of concrete executions.

\emph{Partial order reduction} (POR)~\cite{DBLP:journals/sttt/ClarkeGMP99,DBLP:books/sp/Godefroid96,DBLP:conf/cav/Peled93,DBLP:conf/apn/Valmari89} is an approach that limits the number of explored executions without sacrificing coverage. POR relies on an equivalence relation between executions, which very often means that two executions are equivalent if one can be obtained from the other by swapping consecutive independent (non-conflicting) execution steps. It guarantees that at least one execution from each equivalence class (called a Mazurkiewicz trace~\cite{DBLP:conf/ac/Mazurkiewicz86}) is explored. Optimal POR techniques explore exactly one execution from each equivalence class. Beyond this classic notion of optimality, POR techniques may aim for optimality by avoiding visiting states from which no optimal execution may pass. There is a large body of work on POR techniques that address its soundness when checking a certain class of specifications for a certain class of programs, or its theoretical optimality (see Section~\ref{sec:related}). 
%The set of interleavings explored by some POR technique is defined by restricting the set of threads that are explored from each state (scheduling point). Depending on the class specifications, assumptions about programs, or optimality targets, there are various definitions for this set of processes, including stubborn sets~\cite{DBLP:conf/apn/Valmari89}, persistent sets~\cite{DBLP:books/sp/Godefroid96}, ample sets~\cite{DBLP:journals/sttt/ClarkeGMP99}, and source sets~\cite{DBLP:journals/jacm/AbdullaAJS17}.

We propose \emph{stateless} model checking (SMC) algorithms for database-backed applications that rely on \emph{dynamic} partial order reduction (DPOR). Stateless model checking~\cite{DBLP:conf/popl/Godefroid97} explores executions of a program in a systematic manner without storing visited states, thereby, avoiding excessive memory consumption. Dynamic partial order reduction (DPOR)~\cite{DBLP:conf/popl/FlanaganG05} means that the equivalence relation between executions is tracked on the fly without relying on a-priori static analyses.

\paragraph{Formalizing Isolation Levels}
Our algorithms rely on the axiomatic definitions of isolation
levels introduced by \citet{DBLP:journals/pacmpl/BiswasE19}. These
definitions use logical constraints (called \emph{axioms}) to characterize the
set of executions of a database (key-value store) that conform to a particular isolation
level (this can be extended to SQL queries as shown by \citep{DBLP:journals/pacmpl/BiswasKVEL21}). These constraints refer to a specific set of
relations between events/transactions in an execution that describe control-flow
or data-flow dependencies: a program order $\po$ between events in the same
transaction, a session order $\so$ between transactions in the same session\footnote{A
session is a sequential interface to the storage system. It corresponds to what
is also called a \textit{connection}.}, and a write-read $\wro$ (read-from) relation that
associates each read event with a transaction that writes the value returned by
the read. These relations along with the events (also called operations) in an
execution are called a \emph{history}. 
A history describes only the
interaction with the database, omitting application-side events (e.g., computing
the value to be written to a key). 

\paragraph{Execution Equivalence} DPOR algorithms are parametrized by an equivalence relation on executions, most often, Mazurkiewicz equivalence. In this work, we consider a weaker equivalence relation, also known as \emph{read-from equivalence}~\cite{DBLP:journals/pacmpl/ChalupaCPSV18,DBLP:journals/pacmpl/AbdullaAJN18,DBLP:journals/pacmpl/AbdullaAJLNS19,DBLP:conf/pldi/Kokologiannakis19,DBLP:conf/asplos/Kokologiannakis20,DBLP:journals/pacmpl/Kokologiannakis22}, which considers that two executions are equivalent when their histories are precisely the same (they contain the same set of events, and the relations $\po$, $\so$, and $\wro$ are the same). In general, reads-from equivalence is coarser than Mazurkiewicz equivalence, and its equivalence classes can be exponentially-smaller than Mazurkiewicz traces in certain cases~\cite{DBLP:journals/pacmpl/ChalupaCPSV18}.

\paragraph{SMC Algorithms}
Our SMC algorithms enumerate executions of a given program under a given isolation level $I$. They are \emph{sound}, i.e., enumerate only \emph{feasible} executions (admitted by the program under $I$),  \emph{complete}, i.e., they output a representative of each read-from equivalence class, and \emph{optimal}, i.e., they output \emph{exactly one} complete execution from each read-from equivalence class.
%producing \emph{exactly one} representative from each read-from equivalence class. 
For isolation levels weaker than and including Causal Consistency, they satisfy a notion of \emph{strong optimality} which says that additionally, the enumeration avoids states from which the execution is ``blocked'', i.e., it cannot be extended to a complete execution of the program. 
%This goes beyond the more standard notion of optimality which requires that the algorithm produces exactly one \emph{complete} execution from each equivalence class.
For other isolation levels such as Snapshot Isolation and Serializability, we show that \emph{there exists} no algorithm in the same class (to be discussed below) that can ensure such a strong notion of optimality. Moreover, they are polynomial space, as opposed to many DPOR algorithms proposed in the literature.

We define a generic class of SMC algorithms, called \emph{swapping based}, inspired by the work of \citep{DBLP:conf/pldi/Kokologiannakis19,DBLP:journals/pacmpl/Kokologiannakis22} (which concerns programs running on a shared memory), which track the history of an execution, i.e., the interaction with the database. We assume that the other steps in a transaction besides interacting with the database concern local variables visible only within the scope of the enclosing session.  Executions are extended according to a generic scheduler function $\genericNext$, every read event produces several exploration branches, one for every write executed in the past that it can read from, and events in an execution can be swapped to produce new exploration ``roots'' that lead to different executions. Swapping events is required for completeness, to explore the possibilities where a read $r$ reads from a write $w$ that is scheduled by $\genericNext$ after $r$. We restrict the definition of swapping so that it produces a history that is feasible by construction (extending an execution which is possibly infeasible may violate soundness).

We define a concrete algorithm in this class that in particular, satisfies the stronger notion of optimality mentioned above for every isolation level $I$ which is \emph{prefix-closed} and \emph{causally-extensible}, e.g.,  {\it Read Committed} and \textit{Causal Consistency}. Prefix-closedness means that every prefix of a history that satisfies $I$, i.e., a subset of transactions and all their predecessors in the causal relation, i.e., $(\so\cup\wro)^+$, is also consistent with $I$, and causal extensibility means that any pending transaction in a history that satisfies $I$ can be extended with one more event to still satisfy $I$, and if this is a read event, then, it can read-from a transaction that precedes it in the causal relation. To ensure strong optimality, this algorithm uses a carefully chosen condition for restricting the application of event swaps, which makes the proof of completeness in particular, quite non-trivial.

Isolation levels such as Snapshot Isolation and Serializability are \emph{not} causally-extensible (see Section~\ref{} for counter-examples). We show that there exists no swapping based SMC algorithm which is sound, complete, and strongly optimal at the same time (independent of memory consumption bounds). This impossibility proof uses a program to show that any $\genericNext$ scheduler and any restriction on swaps would violate either completeness or strong optimality. However, we define an extension of the previous algorithm which satisfies the weaker notion of optimality, while preserving soundness, completeness, and polynomial space complexity. This algorithm will simply enumerate executions according to a prefix-closed and causally-extensible isolation level, and filter executions according to the stronger isolation levels Snapshot Isolation and Serializability at the end, before outputting.

We implemented these algorithms in the Java Pathfinder (JPF) model checker~\cite{DBLP:conf/issta/VisserPK04}, and evaluated them on a number of challenging database-backed applications drawn from the literature of distributed systems and databases.  

Our contributions and outline are summarized as follows:
\begin{itemize}
	\item[\S~\ref{}] defines a generic class of swapping based SMC algorithms based on DPOR which are parametrized by a given isolation level.
	\item[\S~\ref{}] identifies a class of isolation levels called prefix-closed and causally-extensible that admit efficient SMC.
	\item[\S~\ref{}] defines a swapping based SMC algorithm which is sound, complete, strongly-optimal, and polynomial space, for any isolation level that is prefix-closed and causally-extensible.
	\item[\S~\ref{}] shows that there exists no swapping based algorithm for Snapshot Isolation and Serializability, which is sound, complete, and strongly-optimal at the same time, and proposes a swapping based algorithm which satisfies ``plain'' optimality (along with soundness, completeness, and polynomial space complexity bounds).
	\item[\S~\ref{}] reports on an implementation and evaluation of these algorithms in the context of a number of interesting case studies.
\end{itemize}

Section~\ref{} recalls the formalization of isolation levels of \citet{DBLP:journals/pacmpl/BiswasE19,DBLP:journals/pacmpl/BiswasKVEL21}, while Sections~\ref{} and~\ref{} conclude with a discussion of related work and concluding remarks.

%========
%
%
%Programming paradigm is in constant evolution, sequential programs tend to easily become obsolete because of its slow performance and even concurrent programs can also be inefficient when the memory requirements increase. The current state-of-the-art tries to overcome those problems by developing parallel programs along with distributed storage systems. However, not every type of application has the same data reliability requirements and therefore developers may want to relax the \textit{isolation level}, i.e. the restrictions imposed to the information stored for guaranteeing consistency, from the database in order to increase performance.
%
%Allowing multiple behaviors in these contexts hinder the already difficult task of verifying concurrent programs. Studying every alternative is something unrealistic, as the number of possible scenarios grows exponentially with the length of the programs. In general, formal methods such as \textcolor{red}{cite examples} are a reasonable approach as they provide certificate of correctness and explainability of the bugs otherwise. Among them, \textit{stateless model checking} (SMC) and \textit{dynamic partial order reduction} (DPOR) \textcolor{red}{cite papers} stand out as the most promising techniques for verifying current programs during the recent years \textcolor{red}{cite papers}.
%
%On one hand, for a given length-bounded program, SMC explores systematically every possible execution without storing at any point the set of already visited ones. On the other hand, DPOR resumes every possible behavior in a more succinct way, reducing the number of executions that have to be explored for covering those behaviors. Henceforth, combining both techniques to obtaining sound, complete and efficient algorithms has been one of the aimed goals in this field and it has been successfully done for concurrent programs with shared memory \textcolor{red}{cite papers}.
%
%Despite their popularity, there is no application of such techniques in parallel programming with distributed memory's literature so far, hence the relevance of filling this gap. Nevertheless, part of the path this paper wants to create is already explored, as shared memory models are not that unrelated with distributed database's. For example, we can mention the relation between \textit{sequential consistency} and \textit{serializability} or \textit{strong release-acquire} and \textit{causal consistency}; where both database isolation level cases are nothing but a generalization of their shared memory counterparts \textcolor{red}{cite papers}.
%
%In this paper, we present STMC, a \textit{sound}, \textit{complete}, \textit{optimal} DPOR algorithm with \textit{linear memory requirements} that employs SMC techniques for verifying some isolation levels. We describe the models that can guarantee those properties, show that \textit{causal consistency} ($\CC$), \textit{read atomic} ($\RA$) and \textit{read committed} ($\RC$) satisfy them and present an example of why more complex models such as \textit{serializability} ($\SER$) cannot be verified with our algorithm. We also present a formal semantics for STMC and exhibit how it evolves from the base algorithm to its current state; requiring executing transactions in isolation and swapping complete blocks of transactions. In addition, we provide some proofs to help the reader having a better understanding of STMC.
%
%On top of this theoretical development, we also furnish this work with an implementation using Java and several benchmarks that study its re. In a nutshell, our software is an extension of JPF \textcolor{red}{cite tool}, a Java-built software analysis framework for Java (parallel) programs. It provides control to DFS traversal of executions, which along its modularity, makes it an ideal tool for developing and extending a database concurrent programs' verifier. In particular, we highlight the easiness for splitting the program memory and database's management and providing an API for writing the programs to analyze. 
%%Our work doesn't finish with the theoretical aspects, as we also furnish this paper with an implementation in Java. In a nutshell, it is an extension of JPF \textcolor{red}{cite tool}, a Java 

%we start from the base algorithm presented in \textcolor{red}{cite Viktor's algorithm} and adapt it to some concrete isolation levels while maintaining its properties  In particular, we describe the properties any model has to ensure for 

%Data storage is no longer about writing data to a single disk with a single point of access. Modern applications require not just data reliability, but also high-throughput concurrent accesses. Applications concerning supply chains, banking, etc. use traditional relational databases for storing and processing data, whereas applications such as social networking software and e-commerce platforms use cloud-based storage systems (such as Azure Cosmos DB \cite{cosmosdb}, Amazon DynamoDB \cite{decandia2007dynamo}, Facebook TAO \cite{facebook-tao}, etc.). We use the term \textit{storage system} in this paper to refer to any such database system or service.

%Providing high-throughput processing, unfortunately, comes at an unavoidable cost of weakening the guarantees offered to users. Concurrently-connected clients may end up observing different views of the same data. These ``anomalies'' can be prevented by using a strong \textit{isolation level} such as \textit{serializability} \cite{DBLP:journals/jacm/Papadimitriou79b}, which essentially offers a single view of the data. However, serializability requires expensive synchronization and incurs a high performance cost. As a consequence, most storage systems use weaker isolation levels, such as {\it Causal Consistency}~\cite{DBLP:journals/cacm/Lamport78,DBLP:conf/sosp/LloydFKA11,antidote}, {\it Snapshot Isolation}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, {\it Read Committed}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, etc. for better performance. In a recent survey of database administrators \cite{survey}, 86\% of the participants responded that most or all of the transactions in their databases execute at Read Committed isolation level.


\begin{comment}


\begin{figure}
	\begin{minipage}{4.2cm}
		\begin{lstlisting}[basicstyle=\ttfamily\footnotesize,escapeinside={(*}{*)},language=MyLang]
			// Append item to cart
			AddItem(item i, userId) {
				begin()
				key = "cart:" + userId
				cart = read(key)
				cart.append(i)
				write(key, cart)
				commit()
			}
		\end{lstlisting}
	\end{minipage}
	\hspace{-5mm}
	\begin{minipage}{4.2cm}
		\begin{lstlisting}[xleftmargin=4mm,basicstyle=\ttfamily\footnotesize,escapeinside={(*}{*)},language=MyLang]
			// Fetch cart and delete item
			DeleteItem(item i, userId) {
				begin()
				key = "cart:" + userId
				cart = read(key)
				cart.remove(i)
				write(key, cart)
				commit()
			}
		\end{lstlisting}
	\end{minipage}
	
	\vspace{-6mm}	
	\resizebox{8.5cm}{!}{
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
			semithick, transform shape]
			\node (s11l) at (1.15, 2.1) {\textbf{Initial state}};
			\node[draw, rounded corners=2mm] (t0) at (2.05, 1.5) {\begin{tabular}{l} $\wrt{\texttt{cart:}u}{\{..\, I\, ..\}}$ \end{tabular}};
			\node[draw, rounded corners=2mm, minimum width=3.6cm, minimum height=1.3cm] (s1) at (0, -0.1) {};
			\node[style={inner sep=0,outer sep=0}] (s11) at (0, 0.3) {\begin{tabular}{l} $\rd{\texttt{cart:}u}{\{..\, I\, ..\}}$\end{tabular}};
			\node[style={inner sep=0,outer sep=0}] (s12) at (0, -0.5) {\begin{tabular}{l} $\wrt{\texttt{cart:}u}{\{..\, I,I\, ..\}}$ \end{tabular}};
			\node (s11l) at (-1, 0.8) {\textbf{AddItem}};
			\node[draw, rounded corners=2mm, minimum width=3.6cm, minimum height=1.3cm] (s2) at (4.1, -0.1) {};
			\node[style={inner sep=0,outer sep=0}] (s21) at (4.1, 0.3) {\begin{tabular}{l} $\rd{\texttt{cart:}u}{\{..\, I\, ..\}}$ \end{tabular}};
			\node[style={inner sep=0,outer sep=0}] (s22) at (4.1, -0.5) {\begin{tabular}{l} $\wrt{\texttt{cart:}u}{\{..\, ..\}}$ \end{tabular}};
			\node (s11l) at (4.9, 0.8) {\textbf{DeleteItem}};
			\node[draw, rounded corners=2mm] (r1) at (8.3, 0) {\begin{tabular}{l} $\rd{\texttt{cart:}u}{\{..\, ..\}}$ \end{tabular}};
			\node[draw, rounded corners=2mm] (r2) at (8.3, -1.3) {\begin{tabular}{l} $\rd{\texttt{cart:}u}{\{..\, I, I\, .\}}$ \end{tabular}};
			\path (s11) edge[left] node {$\po$} (s12);
			\path (s21) edge[left] node {$\po$} (s22);
			\path (t0) edge[left] node {$\wro$} (s1);
			\path (t0) edge[right] node {$\wro$} (s2);
			\path (r1) edge[left] node {$\so$} (r2);
			\path (s2) edge[above] node {$\wro$} (r1);
			\path (s1) edge[below,bend right=11] node {$\wro$} (r2);
			%    \path (t0) edge[red, right, bend left=20] node[pos=0.4,xshift=-1] {$\wro$} (s11);
			%    \path (t0) edge[red, left, bend right=20] node[pos=0.9,xshift=-1] {$\wro$} (s12);
		\end{tikzpicture}  
	}
	
	%  \begin{lstlisting}[xleftmargin=4mm,basicstyle=\ttfamily\footnotesize,escapeinside={(*}{*)},language=MyLang,morekeywords={Test,GetCart}]
	%Test: 
	%{ AddItem(I, u); GetCart(u); GetCart(u) } || DeleteItem(I, u)
	%		\end{lstlisting}
	\vspace{-2mm}
	\caption{A simple shopping cart service.}
	\label{fig:motiv}
	\vspace{-3mm}
\end{figure}

\textcolor{orange}{
%A weaker isolation level allows for more possible behaviors than stronger isolation levels. It is up to the developers then to ensure that their application can tolerate this larger set of behaviors. Unfortunately, weak isolation levels are hard to understand or reason about \cite{DBLP:conf/popl/BrutschyD0V17,adya-thesis} and resulting application bugs can cause loss of business \cite{acidrain}. Consider a simple shopping cart application, inspired from \citet{sivaramakrishnan2015declarative5}, that stores a per-client shopping cart in a key-value store (\textit{key} is the client ID and \textit{value} is a multi-set of items). \figref{motiv} shows procedures for adding an item to the cart (\texttt{AddItem}) and deleting \textit{all} instances of an item from the cart (\texttt{DeleteItem}). Each procedure executes in a transaction, represented by the calls to \texttt{begin} and \texttt{commit}. Suppose that initially, a user $u$ has a single instance of item $I$ in their cart. Then the user connects to the application via two different sessions (for instance, via two browser windows), adds $I$ in one session (\texttt{AddItem($I$, $u$)}) and deletes $I$ in the other session (\texttt{DeleteItem($I$, $u$)}). With serializability, the cart can either be left in the state $\{ I \}$ (delete happened first, followed by the add) or $\emptyset$ (delete happened second). However, with Causal Consistency (or Read Committed), it is possible that with two sequential reads of the shopping cart, the cart is empty in the first read (signaling that the delete has succeeded), but there are \textit{two} instances of $I$  in the second read! Such anomalies, of deleted items reappearing, have been noted in previous work \cite{decandia2007dynamo}. 
}

\paragraph{Testing storage-backed concurrent applications} Even in a serializable context, where the behaviors are limited, verifying a concurrent application is not a trivial task. Since decades researchers have been developed techniques for addressing this problem being the combined application of \textit{Stateless Model Checking} (SMC) and \textit{Dynamic Partial Order Reduction} (DPOR) algorithms the most promising approach. The former is a verification technique that explore all possible executions of a concurrent program without storing the set of states already visited, which avoids excessive memory consumption and improves the scalability factor of its implementations. The latter, on the other hand, refers to the establishment of some equivalence relation between different traces the program may execute. Thanks to these relations we can reduce the set of possible states and, therefore, notably decrease the time consumed.

In this paper we present STMC (Stateless Transactional Model-Checker), a DPOR algorithm for SMC storage-backed concurrent applications, where the multiple threads are only able to communicate between each other via the storage system and there is no transaction that is enable/disabled by any other. This algorithm is inspired in \textcolor{red}{cite Viktor's algorithm} and therefore our result is a \textit{sound}, \textit{complete} and \textit{optimal} algorithm which has a polynomial bound of memory on the program's size. Moreover, our approach is parametric on the storage model, supporting SER, $\CC$ and TRIVIAL models, and takes advantage on the write-read equivalence relation's flexibility to reduce the number of executed traces to those whose \textit{history} are different. \textcolor{blue}{Moreover, we present an alternative version of it focused on pure testing where some executions are discarded in order to, on average, detect earlier when some code is erroneous.}

To achieve this, we build upon prior work \---- most notably, \textcolor{red}{JPF} \---- and represent program executions as \textit{histories}. In summary, we make the following contributions:
\begin{itemize}
	\item We describe through a series of examples the huge difference between the multi-thread (MT) environment and the transactional one.
	\item We describe formally the working context and the notions that appears on it.
	\item We provide a series of algorithms which allow us to understand the resulting products.
	\item We formally prove that STMC is sound, complete and optimal.
	\item We implement a version of STMC, the unique tool at the moment of writing able to solve the problem addressed.
\end{itemize}

%SMC may allow a program to be verified employing polynomial memory and DPOR techniques may reduce exponentially the number of executing traces, but the task of obtaining an algorithm that explore all possible traces according to some semantics (\textit{completeness}) and nothing else (\textit{soundness}) without exploring redundant elements (\textit{optimality}) and using only a polynomial nu

%Thanks to the work of \textcolor{red}{cite things} complete, optimal algorithms that run in polynomial time 

%This paper addresses the problem of \textit{testing} code for correctness against weak behaviors: a developer should be able to write a test that runs their application and then asserts for correct behavior. The main difficulty with testing today is getting coverage of weak behaviors. Running against an actual production storage system is very likely to only result in serializable behaviors because of their optimized implementation. For instance, only 0.0004\% of all reads performed on Facebook's TAO storage system were not serializable \cite{facebook-consistency}. Emulators, offered by cloud providers for local development, on the other hand, do not support weaker isolation levels at all \cite{cosmosdb-local}. Another option, possible when the storage system is available open-source, is to set it up with a tool like Jepsen~\cite{jepsen} to inject noise (bring down replicas or delay packets on the network). This approach is unable to provide good coverage at the level of client operations \cite{clotho} (more details in \sectref{oltp}). Another line of work has focussed on finding anomalies by identifying non-serializable behavior (\sectref{related}). Anomalies, however, do not always correspond to bugs \cite{DBLP:conf/pldi/BrutschyD0V18,isodiff}; they may either not be important (e.g., gathering statistics in a non-serializable fashion may not impact application correctness) or may already be handled by the application (e.g., checking and deleting duplicate items).

\end{comment}