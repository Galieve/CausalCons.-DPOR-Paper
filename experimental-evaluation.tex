\section{Experimental evaluation}

We consider a set of benchmarks inspired in real-world applications and evaluate them under different types of programs and isolation levels; both implemented following the aforementioned specifications. We provide also examples of different behaviors depending on the isolation level for each applications.

\subsection{Applications}

\forceindent
\textit{Shopping Cart}. This application models a web page for shopping. It allows users to add, get and remove items from their shopping cart and modifying the quantities of the items present in the cart. We employ only one table, $\texttt{cart}$, in this application. Given a program that add an item in one section and deletes it in another one, we may observe, depending on the isolation level, that at the end of the execution the cart contains either zero, one or two items.

\textit{Twitter}. Application based on the popular social-network that allow users to follow other users, publish tweets and get their followers, tweets and tweets published by other followers. We model twitter with four tables: $\texttt{users}, \texttt{tweets}, \texttt{followed}, \texttt{followers}$. Under weak isolation levels, it is possible that one user can publish a tweet and not being able to obtain it from a different session. We can also detect other behaviors as users following another users in one session but the latter not being able to find the former as a follower.

\textit{Courseware}. Courseware is an application for managing the enrollment of students in courses in a institution. It allows to open, close and delete courses, enroll students and get all enrollments. One student can only enroll to a course if it is open and its capacity has not reached a fixed limit. It employs three tables, $\texttt{student}, \texttt{course}$ and $\texttt{enrollments}$. Under weak isolation levels, two students in different sessions may enroll to a course with only one free place or being able to enroll into a course that has been deleted in another session.

\textit{Wikipedia}. Application based on the well-known online encyclopedia Wikipedia that allow users to get the content of a page (registered or not), add or remove pages to their watching list and update pages. It employs ten tables, but the vast majority of procedures only access to a small subset of them. Under weak isolation levels, one change in a page may be overwritten by another one done in a different session as well as the watching list may contain a variable number of pages if they are added/deleted from different sessions.

\textit{TPC-C}. TPC-C model any online shopping application with five methods: know the stock of a product, creating a new order, getting its status, paying it and delivering it. TPC-C employs nine tables and all its procedures read and write several variables. Under weak isolation level two orders may be created from different sessions or the account balance may be inconsistent if some order is payed twice.

\subsection{Experiments}

\textit{Experimental setting:} We conducted all experiments on a MacBook Pro 13-inch with $8$ cores and $16$ GB of RAM.

We have run three type of experiments to determine the algorithm's scalability. The more transactions that write into the database, the more potential behaviors allowed. Therefore, in each case we design the experiments with this guideline.

Firstly, for each application we have designed two programs with three threads each and three transactions per thread; where one contains only one transaction that writes per thread instead of the two writing transactions the second program have (in the case of TPC-C we discriminate transactions depending on the number of variables they write). Those programs have been executed for algorithms $\textsc{explore-ce}$ under $\CC$ and $\RA$ and $\textsc{explore-ce}^*$ under the tuples $\langle\RA, \CC\rangle, \langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$. 

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/RA-CC.eps}
		\caption{Evolution of the execution time per benchmark. The running time for the last benchmark is $25' 17''$ under $\RA$ and $33' 3''$ under $\langle\RA, \CC\rangle$ (out of the figure). The execution time of both $\CC, \langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$ coincide.}
		\label{fig:results-ra-cc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Histories.eps}
		\caption{Number of histories per benchmark. For the eight benchmark, we find $12433$ consistent histories under $\RA$, $2309$ under $\CC$ and $1605$ under $\SI$. \\\\}
		\label{fig:results-histories}
	\end{subfigure}

	\caption{Execution of the algorithm under different isolation levels.}
	\label{fig:results1}
\end{figure}

The results obtained in Figure~\ref{fig:results-ra-cc} show that an increment on the number of history a program may have under a concrete isolation level, obliges an increasing execution time. As expected, $\textsc{explore-ce}^*$ under $(\RA, \CC)$ have a worse performance than $\textsc{explore-ce}$ under $\CC$ by the cost of checking if a history satisfies $\CC$. However, as the number of $\CC$ histories is significantly smaller than $\RA$ histories, the cost of $\SI$ and $\SER$ checks is diluted. As $\CC$ is a causal-extensible model and it had a reasonable performance under this experiment, in the following we will work only with this isolation level.
% Moreover, it can be even worse than simply $\textsc{explore-ce}$ under $I_0$ if the $\evaluate$'s cost for isolation level $I$ is higher than the cost of checking if $h$ satisfies $I_0$. Moreover, as seen in Figure~\ref{fig:results-histories}, we notice that even for isolation levels with thousands of histories, the algorithm terminates in a reasonable time (no longer than one hour). As $\CC$ have a reasonable performance under this experiment, in the following we will work only with this isolation level.

Our second experiment measures the algorithm's performance under $\CC$. We design three types of performance situations depending the number of shared variables a pair of transactions update; being ``Light'' a situation where every update is potentially read by another thread, ``Heavy'' one where every update writes a variable every transaction reads and ``Medium'' something in between. For each of them, we describe five TPC-C programs, with a different number of threads each, between $1$ and $5$, with only one transaction per thread.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Threads-time.eps}
		\caption{Evolution of the execution time depending on the number of threads. The running time for the benchmark ``Heavy'' with $5$ threads is $20' 40''$.}
		\label{fig:results-threads}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Threads-histories.eps}
		\caption{Number of histories depending on the number of threads. For the benchmark ``Heavy'', the number of histories when there is $5$ threads is $1296$.}
		\label{fig:results-threads-histories}
	\end{subfigure}
	
	\caption{Execution of the algorithm under $\CC$ with different types of programs; differing in the number of threads and their qualitative description.}
	\label{fig:results2}
\end{figure}

In Figure~\ref{fig:results2} we can observe that, in general, the running time of this algorithm does not exceed the couple of minutes in almost every situation. Moreover, we point that the cost of swapping two events has a sensible impact on the overall performance.

Our third experiment consists on, fixed the number of threads per program, studying how the algorithm performance when those programs have different number of transactions. Analogously to the previous situation, we design three types of performance situations, ``Heavy'', ``Medium'' and ``Light''; and for each of them, we analyze five TPC-C programs with different number of transactions, from $1$ to $5$.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Transactions-time.eps}
		\caption{Evolution of the execution time depending on the number of transactions. The running time for the benchmark ``Heavy'' with $4$ transactions is $10' 58''$ and $29' 31''$ with $5$.}
		\label{fig:results-transactions}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Transactions-histories.eps}
		\caption{Number of histories depending on the number of transactions. The number of histories when there is $5$ transactions is $420$ for the benchmark ``Medium'' and $633$ for the benchmark ``Heavy''.}
		\label{fig:results-transactions-histories}
	\end{subfigure}
	
	\caption{Execution of the algorithm under $\CC$ with different types of programs; differing in the number of transactions and their qualitative description.}
	\label{fig:results3}
\end{figure}

The results in Figure~\ref{fig:results3} supports our claims as the bigger the total space to explore, the greater the time it has to be consumed in the exploration.