%!TEX root = main.tex
\section{Related Work}\label{sec:related}

TODO TO REVISE THE FOLLOWING

There here have been several directions of work addressing the correctness of database-backed applications. 
We directly build upon one line of work concerned with the logical formalization
of isolation levels 
\cite{ansi,DBLP:conf/icde/AdyaLO00,DBLP:conf/sigmod/BerensonBGMOO95,DBLP:conf/concur/Cerone0G15,DBLP:journals/pacmpl/BiswasE19}.
Our work relies on the axiomatic definitions of isolation levels introduced by \citet{DBLP:journals/pacmpl/BiswasE19}, which have also investigated
the problem of checking whether a given history satisfies a certain isolation
level. Our kv-store implementation relies on these algorithms 
to check the validity of the values returned by read operations. Working with a
logical formalization allowed us to avoid implementing an actual database with replication or
sophisticated synchronization.

Another line of work concentrates on the problem of finding ``anomalies'': 
behaviors that are not possible under serializability. This is typically done
via a static analysis of the application code that builds a static dependency graph that
over-approximates the data dependencies in all possible
executions of the application~\cite{DBLP:journals/jacm/CeroneG18,DBLP:conf/concur/0002G16,DBLP:journals/tods/FeketeLOOS05,DBLP:conf/vldb/JorwekarFRS07,acidrain,isodiff}.
Anomalies with respect to a given isolation level then correspond to a
particular class of cycles in this graph. Static dependency graphs turn out to
be highly imprecise in representing feasible executions, leading to false
positives. Another source of false positives is that an anomaly might not be a
bug because the application may already be designed to handle the
non-serializable behavior \cite{DBLP:conf/pldi/BrutschyD0V18,isodiff}. 
Recent work has tried to address these issues by using more precise 
logical encodings of the application,
e.g.~\cite{DBLP:conf/popl/BrutschyD0V17,DBLP:conf/pldi/BrutschyD0V18}, or
by using user-guided heuristics~\cite{isodiff}. 

Another approach consists of modeling the application
logic and the isolation level in first-order logic and relying on SMT solvers to
search for anomalies~\cite{DBLP:journals/pacmpl/KakiESJ18,DBLP:conf/concur/NagarJ18,burcu-netys},
or defining specialized reductions to assertion
checking~\cite{DBLP:conf/concur/BeillahiBE19,DBLP:conf/cav/BeillahiBE19}.
The \textsc{Clotho} tool \cite{clotho}, for instance, uses a static analysis of the application to
generate test cases with plausible anomalies, which are deployed in a concrete
testing environment for generating actual executions. 
Our approach, based on testing with MonkeyDB, has several practical advantages.
There is no need for analyzing application code; we can work with any
application. There are no false positives because we directly run the
application and check for user-defined assertions, instead of looking for
application-agnostic anomalies. The limitation, however, of the MonkeyDB approach is 
the inherent incompleteness of testing.

Several works have looked at the problem of reasoning about the correctness of
applications executing under weak isolation and introducing additional
synchronization when
necessary~\cite{DBLP:conf/eurosys/BalegasDFRPNS15,DBLP:conf/popl/GotsmanYFNS16,DBLP:conf/esop/NairP020,DBLP:conf/usenix/0001LCPRV14}.
As in the previous case, our work based on testing has the advantage that it can
scale to real sized applications (as opposed to these techniques which are based
on static analysis or logical proof arguments), but it cannot prove that an
application is correct. Moreover, the issue of repairing applications is
orthogonal to our work. 

MonkeyDB~\cite{DBLP:journals/pacmpl/BiswasKVEL21}

============

Over the years various different techniques have been introduced to deal with the state explosion problem in model checking. For concurrent programs specifically, depth bounding \cite{DBLP:conf/popl/Godefroid97}, delay bounding \cite{DBLP:conf/popl/EmmiQR11}, context bounding (bounding the number of context switches) \cite{DBLP:conf/tacas/QadeerR05}, preemption bounding \cite{DBLP:conf/pldi/MusuvathiQ07} and phase bounding \cite{DBLP:conf/tacas/BouajjaniE12} bring tractability to the model checking problem and have been shown to be effective for bug finding. These techniques are all {\em incomplete}, in the sense that lack of bugs does not guarantee the correctness of the system.


POR techniques reduce the search space by not exploring multiple executions from the same equivalence class, and are {\em complete}. Early techniques like {\em ample sets} \cite{DBLP:conf/popl/ClarkeES83,DBLP:conf/forte/HolzmannP94} and {\em stubborn sets} \cite{DBLP:journals/fmsd/GodefroidW93,DBLP:conf/dimacs/Godefroid90} were based on static analysis. {\em Sleep sets} \cite{DBLP:conf/dimacs/Godefroid90} were the first to  guarantee optimality (one execution from each equivalence class) \cite{DBLP:journals/fmsd/GodefroidHP95} by keeping track of information from the history of the exploration. However, they only prune transitions and cannot eliminate any state when used alone. Persistent sets \cite{DBLP:conf/cav/GodefroidP93,DBLP:journals/dc/KatzP92} generalized stubborn and ample sets and enabled development of dynamic POR (DPOR) methods.

In \cite{DBLP:conf/popl/FlanaganG05}, an efficient stateful algorithm is proposed for computing persistent sets dynamically by considering  currently explored parts of the state space. This algorithm needed large memory for keeping discovered states and the happens-before relation. The algorithm is improved in \cite{DBLP:conf/spin/YangCGK08} with a more efficient state representation, and in \cite{DBLP:conf/icfem/YiWY06} with a summary-based representation of the happens before. % relation is represented more efficiently by introducing summaries. 

In \cite{DBLP:conf/fase/LauterburgKMA10,DBLP:conf/forte/TasharofiKLLMA12}, stateless dynamic POR techniques were introduced. {\em Source sets} \cite{DBLP:journals/jacm/AbdullaAJS17} were introduced in the context of dynamic POR techniques such that the state space can be reduced up to the limit that is theoretically possible. They are generalizations of persistent sets and their relation with persistent sets are investigated in \cite{DBLP:conf/birthday/AbdullaAJS17}. Our eager and lazy algorithms are relying on source sets but operate in the context of stateful model checking. The technique from \cite{DBLP:conf/atva/NeeleWBP16} is similar to our ample algorithm for the GPU setting, but their choice of invisible actions is different than ours.
While we focus on shared-memory programs running on top of a sequential consistency memory model, POR techniques have been also investigated in the context of weak memory models such as TSO or C11, e.g.,~\cite{DBLP:conf/cav/Kokologiannakis21,DBLP:conf/asplos/Kokologiannakis20,DBLP:journals/acta/AbdullaAAJLS17,DBLP:journals/pacmpl/AbdullaAJN18}.


%AZADEH: WE NEED SOMETHING MORE SOLID ABOUT STATELESS POR. IT CANNOT BE MENTIONED IN A SENTENCE AS A FLEETING THOUGHT ONLY. I WROTE SOMETHING BUT I AM NOT SURE IT IS ACCURATE. I TRIED TO TAKE A FEW RANDOM SENTENCES AND MAKE A THEME OUT OF THEM. READ BELOW AND CORRECT ANY INACCURACIES.


After seminal works like Verisoft~\cite{DBLP:conf/popl/Godefroid97} and Chess [Musuvathi et al. 2008] paved the way for stateless model checking, there has been a large body of work on SMC and DPOR~\cite{DBLP:conf/popl/FlanaganG05}. A major breakthrough in this line of work was made by Abdulla et al.~\cite{DBLP:journals/jacm/AbdullaAJS17}, who provided the first optimal DPOR algorithm for Mazurkiewicz trace equivalence for sequential consistency. This algorithm, as described in 2.3, avoids blocked explorations at the cost of exponential memory consumption. We can broadly classify the more recent works in this area into two main categories depending on their primary focus.
First, many techniques aim to combat the state-space explosion problem by introducing coarser equivalence partitionings\cite{DBLP:journals/pacmpl/AbdullaAJLNS19,DBLP:conf/cav/AgarwalCPPT21,DBLP:conf/tacas/AronisJLS18,DBLP:journals/pacmpl/ChalupaCPSV18,DBLP:journals/pacmpl/ChatterjeePT19}. Among these, only Nidhugg~\cite{DBLP:conf/tacas/AronisJLS18,DBLP:journals/pacmpl/AbdullaAJLNS19} is optimal w.r.t. to its equivalence partitioning(s), although, as demonstrated in 6.1, can suffer from exponential memory consumption.

Second, other techniques focus on extending DPOR to weak memory models either by targeting a specific weak memory model~\cite{DBLP:conf/cav/AbdullaAJL16,DBLP:journals/acta/AbdullaAAJLS17,DBLP:journals/pacmpl/AbdullaAJN18,DBLP:conf/oopsla/NorrisD13} or by being parametric with respect to an axiomatically-defined memory model~\cite{DBLP:conf/pldi/Kokologiannakis19,DBLP:conf/asplos/Kokologiannakis20}. In addition, a number of these works can operate under the coarser reads-from equivalence partitioning, including~\cite{DBLP:journals/pacmpl/AbdullaAJN18} and ~\cite{DBLP:conf/pldi/Kokologiannakis19,DBLP:conf/asplos/Kokologiannakis20}, which do so while maintaining optimality. TruSt builds on the foundations laid by GenMC~\cite{DBLP:conf/pldi/Kokologiannakis19} in order to achieve memory-model parametricity; however, it adapts the extensibility requirement on the underlying memory model to a more precise Å‚maximal extensibility requirement (see 3.1)