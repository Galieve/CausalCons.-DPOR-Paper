%!TEX root = main.tex
\section{Related Work}\label{sec:related}

\noindent
\textbf{Checking Correctness of Database-Backed Applications.}
There here have been several directions of work addressing the correctness of database-backed applications. 
One line of work is concerned with the logical formalization
of isolation levels 
\cite{ansi,DBLP:conf/icde/AdyaLO00,DBLP:conf/sigmod/BerensonBGMOO95,DBLP:conf/concur/Cerone0G15,DBLP:journals/pacmpl/BiswasE19}.
Our work relies on the axiomatic definitions of isolation levels introduced by \citet{DBLP:journals/pacmpl/BiswasE19}, which have also investigated
the problem of checking whether a given history satisfies a certain isolation
level. Our SMC algorithms rely on these algorithms to check consistency of a history with a given isolation level. 
%the validity of the values returned by read operations. Working with a
%logical formalization allowed us to avoid implementing an actual database with replication or
%sophisticated synchronization.

Another line of work concentrates on the problem of finding ``anomalies'': 
behaviors that are not possible under serializability. This is typically done
via a static analysis of the application code that builds a static dependency graph that
over-approximates the data dependencies in all possible
executions of the application~\cite{DBLP:journals/jacm/CeroneG18,DBLP:conf/concur/0002G16,DBLP:journals/tods/FeketeLOOS05,DBLP:conf/vldb/JorwekarFRS07,acidrain,isodiff}.
Anomalies with respect to a given isolation level then correspond to a
particular class of cycles in this graph. Static dependency graphs turn out to
be highly imprecise in representing feasible executions, leading to false
positives. Another source of false positives is that an anomaly might not be a
bug because the application may already be designed to handle the
non-serializable behavior \cite{DBLP:conf/pldi/BrutschyD0V18,isodiff}. 
Recent work has tried to address these issues by using more precise 
logical encodings of the application,
e.g.~\cite{DBLP:conf/popl/BrutschyD0V17,DBLP:conf/pldi/BrutschyD0V18}, or
by using user-guided heuristics~\cite{isodiff}. 
Another approach consists of modeling the application
logic and the isolation level in first-order logic and relying on SMT solvers to
search for anomalies~\cite{DBLP:journals/pacmpl/KakiESJ18,DBLP:conf/concur/NagarJ18,burcu-netys},
or defining specialized reductions to assertion
checking~\cite{DBLP:conf/concur/BeillahiBE19,DBLP:conf/cav/BeillahiBE19}.
Our approach, based on SMC, does not generate false positives because we systematically
enumerate only valid executions of a program which allows to check for user-defined assertions.

Several works have looked at the problem of reasoning about the correctness of
applications executing under weak isolation and introducing additional
synchronization when
necessary~\cite{DBLP:conf/eurosys/BalegasDFRPNS15,DBLP:conf/popl/GotsmanYFNS16,DBLP:conf/esop/NairP020,DBLP:conf/usenix/0001LCPRV14}.
These are based on static analysis or logical proof arguments. 
The issue of repairing applications is orthogonal to our work. 

MonkeyDB~\cite{DBLP:journals/pacmpl/BiswasKVEL21} is a mock storage system for testing storage-backed applications. 
While being able to scale to larger application code, it has the inherent incompleteness of testing.


%The \textsc{Clotho} tool \cite{clotho}, for instance, uses a static analysis of the application to
%generate test cases with plausible anomalies, which are deployed in a concrete
%testing environment for generating actual executions. 
%
%
%Our approach, based on testing with MonkeyDB, has several practical advantages.
%There is no need for analyzing application code; we can work with any
%application. There are no false positives because we directly run the
%application and check for user-defined assertions, instead of looking for
%application-agnostic anomalies. The limitation, however, of the MonkeyDB approach is 
%the inherent incompleteness of testing.
%
%
%As in the previous case, our work based on testing has the advantage that it can
%scale to real sized applications (as opposed to these techniques which are based
%on static analysis or logical proof arguments), but it cannot prove that an
%application is correct. Moreover, the issue of repairing applications is
%orthogonal to our work. 
%
%MonkeyDB~\cite{DBLP:journals/pacmpl/BiswasKVEL21}

%\vspace{1mm}
%\noindent
%\textbf{Model Checking Concurrent Programs.}
%Over the years various different techniques have been introduced to deal with the state explosion problem in model checking. For concurrent programs specifically, depth bounding \cite{DBLP:conf/popl/Godefroid97}, delay bounding \cite{DBLP:conf/popl/EmmiQR11}, context bounding (bounding the number of context switches) \cite{DBLP:conf/tacas/QadeerR05}, preemption bounding \cite{DBLP:conf/pldi/MusuvathiQ07} and phase bounding \cite{DBLP:conf/tacas/BouajjaniE12} bring tractability to the model checking problem and have been shown to be effective for bug finding. These techniques are all {\em incomplete}, in the sense that lack of bugs does not guarantee the correctness of the system.

\vspace{1mm}
\noindent
\textbf{Partial Order Reduction.}
POR techniques deal with the state explosion problem in model checking by not exploring multiple executions from the same equivalence class, and are {\em complete}. Early techniques like {\em ample sets} \cite{DBLP:conf/popl/ClarkeES83,DBLP:conf/forte/HolzmannP94} and {\em stubborn sets} \cite{DBLP:journals/fmsd/GodefroidW93,DBLP:conf/dimacs/Godefroid90} were based on static analysis. {\em Sleep sets} \cite{DBLP:conf/dimacs/Godefroid90} were the first to  guarantee optimality (one execution from each Mazurkiewicz trace) \cite{DBLP:journals/fmsd/GodefroidHP95} by keeping track of information from the history of the exploration. %However, they only prune transitions and cannot eliminate any state when used alone. 
Persistent sets \cite{DBLP:conf/cav/GodefroidP93,DBLP:journals/dc/KatzP92} generalized stubborn and ample sets and enabled development of dynamic POR (DPOR) methods.

\vspace{1mm}
\noindent
\textbf{Dynamic Partial Order Reduction.}
DPOR~\cite{DBLP:conf/popl/FlanaganG05} algorithms explore the execution space (and tracking the equivalence relation) on-the-fly without relying on a-priori static analyses. This is typically coupled with SMC~\cite{DBLP:conf/popl/Godefroid97} which explores executions of a program without storing visited states, thereby, avoiding excessive memory consumption. 

\citet{DBLP:journals/jacm/AbdullaAJS17} introduced the concept of \emph{source sets} which provided the first strongly optimal DPOR algorithm for Mazurkiewicz trace equivalence. Other works study DPOR techniques for equivalence relations which are coarser than Mazurkiewicz traces, e.g.,~\cite{DBLP:journals/pacmpl/AbdullaAJLNS19,DBLP:conf/cav/AgarwalCPPT21,DBLP:conf/tacas/AronisJLS18,DBLP:journals/pacmpl/ChalupaCPSV18,DBLP:journals/pacmpl/ChatterjeePT19}. In all cases, the space complexity is exponential when strong optimality is ensured. % but consumes exponential memory.

%First, many techniques aim to combat the state-space explosion problem by introducing coarser equivalence partitioning\cite{DBLP:journals/pacmpl/AbdullaAJLNS19,DBLP:conf/cav/AgarwalCPPT21,DBLP:conf/tacas/AronisJLS18,DBLP:journals/pacmpl/ChalupaCPSV18,DBLP:journals/pacmpl/ChatterjeePT19}. Among these, only Nidhugg~\cite{DBLP:conf/tacas/AronisJLS18,DBLP:journals/pacmpl/AbdullaAJLNS19} is optimal w.r.t. to its equivalence partitioning(s), although, as demonstrated in 6.1, can suffer from exponential memory consumption.

Other works focus on extending DPOR to weak memory models either by targeting a specific memory model~\cite{DBLP:conf/cav/AbdullaAJL16,DBLP:journals/acta/AbdullaAAJLS17,DBLP:journals/pacmpl/AbdullaAJN18,DBLP:conf/oopsla/NorrisD13} or by being parametric with respect to an axiomatically-defined memory model~\cite{DBLP:conf/pldi/Kokologiannakis19,DBLP:conf/asplos/Kokologiannakis20,DBLP:journals/pacmpl/Kokologiannakis22}. Some of these works can deal with the coarser reads-from equivalence, e.g.,~\cite{DBLP:journals/pacmpl/AbdullaAJN18,DBLP:conf/pldi/Kokologiannakis19,DBLP:conf/asplos/Kokologiannakis20,DBLP:journals/pacmpl/Kokologiannakis22}. Our algorithms build on the work of \citet{DBLP:journals/pacmpl/Kokologiannakis22} which for the first time, proposes a DPOR algorithm which is both strongly optimal and polynomial space. However, the definitions of database isolation levels are quite different with respect to weak memory models, which makes these previous works not extensible in a direct manner. These definitions include a semantics for \emph{transactions} which are collections of reads and writes, and this poses new difficult challenges. 
For instance, reasoning about the completeness and the (strong) optimality of existing DPOR algorithms for shared-memory is agnostic to the scheduler (the $\genericNext$ function in our context) while the strong optimality of our $\textsc{explore-ce}$ algorithm relies on the scheduler keeping at most one transaction pending at a time. This makes our completeness and optimality proofs radically different. 
Moreover, we identify isolation levels ($\SI$ and $\SER$) for which it is impossible to ensure both strong optimality and polynomial space bounds (at least with a swapping-based algorithm), a type of question that has not been investigated in previous work (up to our knowledge). 

%investigate the question of whether it is possible to ensure both strong optimality and polynomial space bounds for certain isolation levels ($\SI$ and $\SER$)
%
%In general, completeness and optimality proofs are very sensitive 


%TruSt builds on the foundations laid by GenMC~\cite{DBLP:conf/pldi/Kokologiannakis19} in order to achieve memory-model parametricity; however, it adapts the extensibility requirement on the underlying memory model to a more precise łmaximal extensibility requirement (see 3.1)


%In \cite{DBLP:conf/popl/FlanaganG05}, an efficient stateful algorithm is proposed for computing persistent sets dynamically by considering  currently explored parts of the state space. This algorithm needed large memory for keeping discovered states and the happens-before relation. The algorithm is improved in \cite{DBLP:conf/spin/YangCGK08} with a more efficient state representation, and in \cite{DBLP:conf/icfem/YiWY06} with a summary-based representation of the happens before. % relation is represented more efficiently by introducing summaries. 
%
%In \cite{DBLP:conf/fase/LauterburgKMA10,DBLP:conf/forte/TasharofiKLLMA12}, stateless dynamic POR techniques were introduced. {\em Source sets} \cite{DBLP:journals/jacm/AbdullaAJS17} were introduced in the context of dynamic POR techniques such that the state space can be reduced up to the limit that is theoretically possible. They are generalizations of persistent sets and their relation with persistent sets are investigated in \cite{DBLP:conf/birthday/AbdullaAJS17}. Our eager and lazy algorithms are relying on source sets but operate in the context of stateful model checking. The technique from \cite{DBLP:conf/atva/NeeleWBP16} is similar to our ample algorithm for the GPU setting, but their choice of invisible actions is different than ours.
%While we focus on shared-memory programs running on top of a sequential consistency memory model, POR techniques have been also investigated in the context of weak memory models such as TSO or C11, e.g.,~\cite{DBLP:conf/cav/Kokologiannakis21,DBLP:conf/asplos/Kokologiannakis20,DBLP:journals/acta/AbdullaAAJLS17,DBLP:journals/pacmpl/AbdullaAJN18}.


%AZADEH: WE NEED SOMETHING MORE SOLID ABOUT STATELESS POR. IT CANNOT BE MENTIONED IN A SENTENCE AS A FLEETING THOUGHT ONLY. I WROTE SOMETHING BUT I AM NOT SURE IT IS ACCURATE. I TRIED TO TAKE A FEW RANDOM SENTENCES AND MAKE A THEME OUT OF THEM. READ BELOW AND CORRECT ANY INACCURACIES.


%After seminal works like Verisoft~\cite{DBLP:conf/popl/Godefroid97} and Chess [Musuvathi et al. 2008] paved the way for stateless model checking, there has been a large body of work on SMC and DPOR~\cite{DBLP:conf/popl/FlanaganG05}. A major breakthrough in this line of work was made by Abdulla et al.~\cite{DBLP:journals/jacm/AbdullaAJS17}, who provided the first optimal DPOR algorithm for Mazurkiewicz trace equivalence for sequential consistency. This algorithm, as described in 2.3, avoids blocked explorations at the cost of exponential memory consumption. We can broadly classify the more recent works in this area into two main categories depending on their primary focus.
%First, many techniques aim to combat the state-space explosion problem by introducing coarser equivalence partitionings\cite{DBLP:journals/pacmpl/AbdullaAJLNS19,DBLP:conf/cav/AgarwalCPPT21,DBLP:conf/tacas/AronisJLS18,DBLP:journals/pacmpl/ChalupaCPSV18,DBLP:journals/pacmpl/ChatterjeePT19}. Among these, only Nidhugg~\cite{DBLP:conf/tacas/AronisJLS18,DBLP:journals/pacmpl/AbdullaAJLNS19} is optimal w.r.t. to its equivalence partitioning(s), although, as demonstrated in 6.1, can suffer from exponential memory consumption.
%
%Second, other techniques focus on extending DPOR to weak memory models either by targeting a specific weak memory model~\cite{DBLP:conf/cav/AbdullaAJL16,DBLP:journals/acta/AbdullaAAJLS17,DBLP:journals/pacmpl/AbdullaAJN18,DBLP:conf/oopsla/NorrisD13} or by being parametric with respect to an axiomatically-defined memory model~\cite{DBLP:conf/pldi/Kokologiannakis19,DBLP:conf/asplos/Kokologiannakis20}. In addition, a number of these works can operate under the coarser reads-from equivalence partitioning, including~\cite{DBLP:journals/pacmpl/AbdullaAJN18} and ~\cite{DBLP:conf/pldi/Kokologiannakis19,DBLP:conf/asplos/Kokologiannakis20}, which do so while maintaining optimality. TruSt builds on the foundations laid by GenMC~\cite{DBLP:conf/pldi/Kokologiannakis19} in order to achieve memory-model parametricity; however, it adapts the extensibility requirement on the underlying memory model to a more precise łmaximal extensibility requirement (see 3.1)